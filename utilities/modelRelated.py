from typing import Dict, List, Optional, Any, TypedDict, Annotated
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage
import os
import time


def invoke_model(model_name : str, messages : List[BaseMessage], temperature: float = 0.2) -> str:
    """è°ƒç”¨å¤§æ¨¡å‹"""
    print(f"ğŸš€ å¼€å§‹è°ƒç”¨LLM: {model_name} (temperature={temperature})")
    start_time = time.time()
    if model_name.startswith("gpt-"):  # ChatGPT ç³»åˆ—æ¨¡å‹
        print("ğŸ” ä½¿ç”¨ OpenAI ChatGPT æ¨¡å‹")
        base_url = "https://api.openai.com/v1"
        api_key = os.getenv("OPENAI_API_KEY")
    else:  # å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚ deepseek, siliconflow...
        print("ğŸ” ä½¿ç”¨ SiliconFlow æ¨¡å‹")
        base_url = "https://api.siliconflow.cn/v1"
        api_key = os.getenv("SILICONFLOW_API_KEY")
    llm = ChatOpenAI(
        model = model_name,
        api_key=api_key, 
        base_url=base_url,
        streaming=True,
        temperature=temperature
    )

    full_response = ""

    try:
        total_tokens_used = {"input": 0, "output": 0, "total": 0}
        
        for chunk in llm.stream(messages):
            chunk_content = chunk.content
            print(chunk_content, end="", flush=True)
            full_response += chunk_content
            
            # Extract token usage if available in chunk
            if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                usage = chunk.usage_metadata
                total_tokens_used["input"] = usage.get('input_tokens', 0)
                total_tokens_used["output"] = usage.get('output_tokens', 0)
                total_tokens_used["total"] = usage.get('total_tokens', 0)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Print timing and token usage
        print(f"\nâ±ï¸ LLMè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        if total_tokens_used["total"] > 0:
            print(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={total_tokens_used['input']:,} | è¾“å‡º={total_tokens_used['output']:,} | æ€»è®¡={total_tokens_used['total']:,}")
        
    except Exception as e:
        end_time = time.time()
        execution_time = end_time - start_time
        print(f"\nâŒ LLMè°ƒç”¨å¤±è´¥ï¼Œè€—æ—¶: {execution_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
        
        # Print any token usage that was captured before failure
        if total_tokens_used["total"] > 0:
            print(f"ğŸ“Š å¤±è´¥å‰Tokenä½¿ç”¨: è¾“å…¥={total_tokens_used['input']:,} | è¾“å‡º={total_tokens_used['output']:,} | æ€»è®¡={total_tokens_used['total']:,}")
        
        raise
    
    return full_response

def invoke_model_with_tools(model_name : str, messages : List[BaseMessage], tools : List[str], temperature: float = 0.2) -> Any:
    """è°ƒç”¨å¤§æ¨¡å‹å¹¶ä½¿ç”¨å·¥å…·"""
    print(f"ğŸš€ å¼€å§‹è°ƒç”¨LLM(å¸¦å·¥å…·): {model_name} (temperature={temperature})")
    start_time = time.time()
    
    
    
    if model_name.startswith("gpt-"):  # ChatGPT ç³»åˆ—æ¨¡å‹
        print("ğŸ” ä½¿ç”¨ OpenAI ChatGPT æ¨¡å‹")
        base_url = "https://api.openai.com/v1"
        api_key = os.getenv("OPENAI_API_KEY")
    else:  # å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚ deepseek, siliconflow...
        print("ğŸ” ä½¿ç”¨ SiliconFlow æ¨¡å‹")
        base_url = "https://api.siliconflow.cn/v1"
        api_key = os.getenv("SILICONFLOW_API_KEY")

    llm = ChatOpenAI(
        model=model_name,
        api_key=api_key,
        base_url=base_url,
        streaming=False,
        temperature=temperature
    )
    
    try:
        # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
        llm_with_tools = llm.bind_tools(tools)
        
        print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLM...")
        
        response = llm_with_tools.invoke(messages)
        
        print("ğŸ“¥ LLMå“åº”æ¥æ”¶å®Œæˆ")
        
        # æ‰“å°å“åº”å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if response.content:
            print(f"\nğŸ’¬ LLMå›å¤å†…å®¹:")
            print(response.content)
        
        # Extract token usage information
        token_usage = {"input": 0, "output": 0, "total": 0, "reasoning": 0}
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage = response.usage_metadata
            token_usage["input"] = usage.get('input_tokens', 0)
            token_usage["output"] = usage.get('output_tokens', 0)
            token_usage["total"] = usage.get('total_tokens', 0)
            
            # Check for reasoning tokens (for reasoning models like Qwen3-32B)
            if 'output_token_details' in usage and usage['output_token_details']:
                token_usage["reasoning"] = usage['output_token_details'].get('reasoning', 0)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(response, 'tool_calls') and response.tool_calls:
            print(f"\nğŸ”§ æ£€æµ‹åˆ° {len(response.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨:")
            
            # æ‰“å°æ¯ä¸ªå·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
            for i, tool_call in enumerate(response.tool_calls):
                print(f"\nğŸ“‹ å·¥å…·è°ƒç”¨ {i+1}:")
                print(f"   ğŸ”§ å·¥å…·åç§°: {tool_call.get('name', 'unknown')}")
                
                # æå–å·¥å…·å‚æ•°
                args = tool_call.get('args', {})
                print(f"   ğŸ“ å‚æ•°: {args}")
                
                # å¦‚æœæ˜¯ç”¨æˆ·äº¤äº’å·¥å…·ï¼Œç‰¹åˆ«æ˜¾ç¤ºé—®é¢˜
                if tool_call.get('name') == 'request_user_clarification':
                    question = args.get('question', '')
                    context = args.get('context', '')
                    if question:
                        print(f"\nğŸ’¬ â­ ç”¨æˆ·é—®é¢˜: {question}")
                        if context:
                            print(f"ğŸ“– ä¸Šä¸‹æ–‡: {context}")
                elif tool_call.get('name') == '_collect_user_input':
                    print(f"\nğŸ”„ å°†æ”¶é›†ç”¨æˆ·è¾“å…¥ä¿¡æ¯")
                    session_id = args.get('session_id', '')
                    if session_id:
                        print(f"ğŸ“‹ ä¼šè¯ID: {session_id}")
            
            end_time = time.time()
            execution_time = end_time - start_time
            print(f"\nâ±ï¸ LLMè°ƒç”¨å®Œæˆ(å¸¦å·¥å…·è°ƒç”¨)ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        else:
            end_time = time.time()
            execution_time = end_time - start_time
            print(f"\nâ±ï¸ LLMè°ƒç”¨å®Œæˆ(æ— å·¥å…·è°ƒç”¨)ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        
        # Print token usage information
        if token_usage["total"] > 0:
            print(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={token_usage['input']:,} | è¾“å‡º={token_usage['output']:,} | æ€»è®¡={token_usage['total']:,}")
            if token_usage["reasoning"] > 0:
                print(f"ğŸ§  æ¨ç†Token: {token_usage['reasoning']:,} (å†…éƒ¨æ¨ç†è¿‡ç¨‹)")
                visible_output = token_usage["output"] - token_usage["reasoning"]
                print(f"ğŸ‘€ å¯è§è¾“å‡ºToken: {visible_output:,}")
        else:
            print("âš ï¸ æœªèƒ½è·å–Tokenä½¿ç”¨ä¿¡æ¯")
        
        # è¿”å›å®Œæ•´å“åº”ä»¥ä¾¿è°ƒç”¨è€…å¤„ç†
        return response
        
    except Exception as e:
        end_time = time.time()
        execution_time = end_time - start_time
        print(f"\nâŒ LLMè°ƒç”¨å¤±è´¥ï¼Œè€—æ—¶: {execution_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
        
        # Try to extract token usage even from failed requests
        if 'response' in locals() and hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage = response.usage_metadata
            input_tokens = usage.get('input_tokens', 0)
            output_tokens = usage.get('output_tokens', 0)
            total_tokens = usage.get('total_tokens', 0)
            print(f"ğŸ“Š å¤±è´¥å‰Tokenä½¿ç”¨: è¾“å…¥={input_tokens:,} | è¾“å‡º={output_tokens:,} | æ€»è®¡={total_tokens:,}")
        
        import traceback
        traceback.print_exc()
        raise